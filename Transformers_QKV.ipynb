{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdux2MuUcMvJCbbue5YbRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrihariR2004/Basics_Transformers/blob/main/Transformers_QKV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bqEEALv-I3Yn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I love coding.\",\n",
        "    \"You enjoy AI.\",\n",
        "    \"AI is amazing.\",\n",
        "    \"Coding is fun.\",\n",
        "    \"I enjoy learning.\"\n",
        "]"
      ],
      "metadata": {
        "id": "-qNsdZ8qJnpB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters\n"
      ],
      "metadata": {
        "id": "RAy-KHJkNTCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 4  # Embedding size\n",
        "head_dim = 2       # Attention head size\n",
        "np.random.seed(42)  # To make the example reproducible\n"
      ],
      "metadata": {
        "id": "guhNq9oFNUPe"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random embeddings for each word"
      ],
      "metadata": {
        "id": "z6Usa-BsNa4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(word):\n",
        "    return np.random.rand(embedding_dim)"
      ],
      "metadata": {
        "id": "l-IpGYhbNb7l"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = [\n",
        "    \"I\", \"love\", \"coding\", \"You\", \"enjoy\", \"AI\", \"is\", \"amazing\", \"fun\", \"learning\"\n",
        "]"
      ],
      "metadata": {
        "id": "L9qdd-DrQKC8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Preprocess the sentences (tokenization & embeddings)**"
      ],
      "metadata": {
        "id": "CRWIVifFNhC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    return np.array([generate_embeddings(word) for word in words])"
      ],
      "metadata": {
        "id": "H_RJN6q6Ndjq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert sentences to embeddings (list of matrices)"
      ],
      "metadata": {
        "id": "Q-ing1u3NuKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [preprocess_sentence(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "KRaoHzJ8NrWi"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random weight matrices for Q, K, V"
      ],
      "metadata": {
        "id": "rzCLAzvKN1Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_q = np.random.rand(embedding_dim, head_dim)  # Q matrix\n",
        "W_k = np.random.rand(embedding_dim, head_dim)  # K matrix\n",
        "W_v = np.random.rand(embedding_dim, head_dim)  # V matrix"
      ],
      "metadata": {
        "id": "dFPyi663Nwnm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Output weight matrix"
      ],
      "metadata": {
        "id": "TcJ0Q7UgN5EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_vocab = np.random.rand(head_dim, embedding_dim)"
      ],
      "metadata": {
        "id": "uBejVQkGNymZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Apply self-attention mechanism to each sentence**"
      ],
      "metadata": {
        "id": "1fp2skYqOEo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_attention(X):\n",
        "    Q = X @ W_q  # shape (num_tokens, head_dim)\n",
        "    K = X @ W_k  # shape (num_tokens, head_dim)\n",
        "    V = X @ W_v  # shape (num_tokens, head_dim)\n",
        "    # Compute attention scores (scaled dot-product attention)\n",
        "    scores = Q @ K.T / math.sqrt(head_dim)  # shape (num_tokens, num_tokens)\n",
        "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # softmax\n",
        "\n",
        "    # Compute the attention output\n",
        "    attention_output = attention_weights @ V  # shape (num_tokens, head_dim)\n",
        "\n",
        "    return attention_output"
      ],
      "metadata": {
        "id": "8DuozwpdOCH1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Perform attention on each sentence and predict next word**"
      ],
      "metadata": {
        "id": "GNahYMOeOTp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence_embeddings in enumerate(X):\n",
        "    attention_output = apply_attention(sentence_embeddings)\n",
        "    final_vector = attention_output[-1]  # Use the output of the last word in the sentence\n",
        "\n",
        "    # Compute logits for the next word prediction (map back to original word space)\n",
        "    # Original: logits = final_vector @ W_vocab.T  # shape (embedding_dim,)\n",
        "    # Changed: Transpose final_vector to match W_vocab's dimensions for multiplication\n",
        "\n",
        "    logits = final_vector.T @ W_vocab # shape (embedding_dim,)\n",
        "    probs = np.exp(logits) / np.sum(np.exp(logits))  # softmax to get probabilities\n",
        "\n",
        "    # Show results\n",
        "    print(f\"Sentence: {sentences[i]}\")\n",
        "    print(\"Next word probabilities (normalized):\")\n",
        "    print(probs)\n",
        "\n",
        "    # Since we don't have a vocabulary list here, we'll pick the highest-probability word\n",
        "    predicted_word = f\"Predicted Next Word (index {np.argmax(probs)})\"\n",
        "    print(f\"Predicted Next Word: {predicted_word}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYoKBWc0OOu3",
        "outputId": "ff9d0dd9-586c-47f0-d5ab-fa917125f118"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I love coding.\n",
            "Next word probabilities (normalized):\n",
            "[0.22524353 0.19621359 0.27253067 0.30601221]\n",
            "Predicted Next Word: Predicted Next Word (index 3)\n",
            "\n",
            "Sentence: You enjoy AI.\n",
            "Next word probabilities (normalized):\n",
            "[0.229503   0.2066897  0.26997677 0.29383052]\n",
            "Predicted Next Word: Predicted Next Word (index 3)\n",
            "\n",
            "Sentence: AI is amazing.\n",
            "Next word probabilities (normalized):\n",
            "[0.2345632  0.18701278 0.2558757  0.32254833]\n",
            "Predicted Next Word: Predicted Next Word (index 3)\n",
            "\n",
            "Sentence: Coding is fun.\n",
            "Next word probabilities (normalized):\n",
            "[0.23592466 0.20990959 0.26207982 0.29208592]\n",
            "Predicted Next Word: Predicted Next Word (index 3)\n",
            "\n",
            "Sentence: I enjoy learning.\n",
            "Next word probabilities (normalized):\n",
            "[0.2232394  0.18291714 0.26983762 0.32400583]\n",
            "Predicted Next Word: Predicted Next Word (index 3)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(sentence_embeddings):\n",
        "    attention_output = apply_attention(sentence_embeddings)\n",
        "    final_vector = attention_output[-1]  # Use the output of the last word in the sentence\n",
        "\n",
        "    # Transpose final_vector instead of W_vocab to match the dimensions\n",
        "    logits = final_vector.T @ W_vocab  # shape (embedding_dim,)\n",
        "\n",
        "    probs = np.exp(logits) / np.sum(np.exp(logits))  # softmax to get probabilities\n",
        "\n",
        "    # Return the index of the word with the highest probability\n",
        "    return np.argmax(probs)"
      ],
      "metadata": {
        "id": "2_Cz9wlMOYQv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Predict for a new sentence**"
      ],
      "metadata": {
        "id": "yEOBbNNpPX_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = \"I enjoy coding\"\n",
        "\n",
        "# Preprocess the new sentence (convert to embeddings)\n",
        "new_sentence_embeddings = preprocess_sentence(new_sentence)\n",
        "\n",
        "# Predict next word for the new sentence\n",
        "predicted_index = predict_next_word(new_sentence_embeddings)\n",
        "\n",
        "# Output the result as a word (instead of index)\n",
        "predicted_word = vocabulary[predicted_index]\n",
        "\n",
        "# Display the results\n",
        "print(f\"New Sentence: {new_sentence}\")\n",
        "print(f\"Predicted Next Word: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O3GPsfrPVvB",
        "outputId": "11b56424-5202-4d84-b8ca-f30e18317cea"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Sentence: I enjoy coding\n",
            "Predicted Next Word: You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-Hxgn3oPbOI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}